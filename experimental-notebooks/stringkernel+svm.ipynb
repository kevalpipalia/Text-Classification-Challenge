{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa97fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Importing custom utility functions\n",
    "from utilities.data_loader import load_modeling_data, load_testing_data, prepare_kaggle_submission\n",
    "from utilities.text_cleaner import advanced_data_cleaning\n",
    "\n",
    "# Importing modeling utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c98d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = load_modeling_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f5042a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = list(train_data['text'].apply(str.split).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c8e6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=train_sentences, \n",
    "                 sg=1, \n",
    "                 workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad280105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multistop',\n",
       " 'i',\n",
       " 'exclamationMark',\n",
       " 'to',\n",
       " 'the',\n",
       " 'a',\n",
       " 'is',\n",
       " 'it',\n",
       " 'my',\n",
       " 'you',\n",
       " 'and',\n",
       " 'not',\n",
       " 'in',\n",
       " 'for',\n",
       " 'questionMark',\n",
       " 'am',\n",
       " 'of',\n",
       " 'have',\n",
       " 'on',\n",
       " 'me',\n",
       " 'that',\n",
       " 'so',\n",
       " 'but',\n",
       " 'just',\n",
       " 'do',\n",
       " 'with',\n",
       " 'be',\n",
       " 'are',\n",
       " 'at',\n",
       " 'wa',\n",
       " 'day',\n",
       " 'will',\n",
       " 'this',\n",
       " 'now',\n",
       " 'can',\n",
       " 'good',\n",
       " 'up',\n",
       " 'going',\n",
       " 'get',\n",
       " 'no',\n",
       " 'all',\n",
       " 'out',\n",
       " 'like',\n",
       " 'go',\n",
       " 'u',\n",
       " 'http',\n",
       " 'work',\n",
       " 'today',\n",
       " 'love',\n",
       " 'time',\n",
       " 'too',\n",
       " 'want',\n",
       " 'your',\n",
       " 'we',\n",
       " 'got',\n",
       " 'what',\n",
       " 'lol',\n",
       " 'know',\n",
       " 'one',\n",
       " 'back',\n",
       " 'from',\n",
       " 'com',\n",
       " 'im',\n",
       " 'about',\n",
       " 'really',\n",
       " 'night',\n",
       " 'had',\n",
       " 'there',\n",
       " 'see',\n",
       " 'did',\n",
       " 'some',\n",
       " 'andamp',\n",
       " 'how',\n",
       " 'andquot',\n",
       " 'if',\n",
       " 'they',\n",
       " 'think',\n",
       " 'still',\n",
       " 'well',\n",
       " 'new',\n",
       " 'would',\n",
       " 'need',\n",
       " 'ha',\n",
       " 'thanks',\n",
       " 'home',\n",
       " 'he',\n",
       " 'oh',\n",
       " 'when',\n",
       " 'miss',\n",
       " 'here',\n",
       " 'more',\n",
       " 'much',\n",
       " 'off',\n",
       " 'last',\n",
       " 'morning',\n",
       " 'an',\n",
       " 'feel',\n",
       " 'hope',\n",
       " 'then',\n",
       " 'make',\n",
       " 'haha',\n",
       " 'twitter',\n",
       " 'tomorrow',\n",
       " 'been',\n",
       " 'great',\n",
       " 'or',\n",
       " 'her',\n",
       " 'again',\n",
       " 'wish',\n",
       " 'she',\n",
       " 'sad',\n",
       " 'come',\n",
       " 'fun',\n",
       " 'week',\n",
       " 'why',\n",
       " 'sleep',\n",
       " 'only',\n",
       " 'right',\n",
       " 'bad',\n",
       " 'very',\n",
       " 'could',\n",
       " 'thing',\n",
       " 'happy',\n",
       " 'tonight',\n",
       " 'sorry',\n",
       " 'friend',\n",
       " 'by',\n",
       " 'way',\n",
       " 'bit',\n",
       " 'them',\n",
       " 'getting',\n",
       " 'should',\n",
       " 'look',\n",
       " 'though',\n",
       " 'yeah',\n",
       " 'nice',\n",
       " 'over',\n",
       " 'better',\n",
       " 'watching',\n",
       " 'yes',\n",
       " 'say',\n",
       " 'wait',\n",
       " 'bed',\n",
       " 'hate',\n",
       " 'cute',\n",
       " 'school',\n",
       " 'people',\n",
       " 'hour',\n",
       " 'twitpic',\n",
       " 'doe',\n",
       " 'x',\n",
       " 'guy',\n",
       " 'weekend',\n",
       " 'him',\n",
       " 'even',\n",
       " 'tweet',\n",
       " 'hey',\n",
       " 'show',\n",
       " 'after',\n",
       " 'who',\n",
       " 'take',\n",
       " 'down',\n",
       " 'were',\n",
       " 'awesome',\n",
       " 'next',\n",
       " 'never',\n",
       " 'life',\n",
       " 'soon',\n",
       " 'thank',\n",
       " 'cant',\n",
       " 'dont',\n",
       " 'let',\n",
       " 'p',\n",
       " 'long',\n",
       " 'working',\n",
       " 'andlt',\n",
       " 'first',\n",
       " 'little',\n",
       " 'year',\n",
       " 'movie',\n",
       " 'best',\n",
       " 'please',\n",
       " 'everyone',\n",
       " 'girl',\n",
       " 'tired',\n",
       " 'doing',\n",
       " 'having',\n",
       " 'being',\n",
       " 'ok',\n",
       " 'watch',\n",
       " 'sick',\n",
       " 'yay',\n",
       " 'feeling',\n",
       " 'his',\n",
       " 'done',\n",
       " 'because',\n",
       " 'any',\n",
       " 'our',\n",
       " 'always',\n",
       " 'n',\n",
       " 'ah',\n",
       " 'suck',\n",
       " 'where',\n",
       " 'sure',\n",
       " 'already',\n",
       " 'cool',\n",
       " 'lot',\n",
       " 'another',\n",
       " 'find',\n",
       " 'than',\n",
       " 'something',\n",
       " 'phone',\n",
       " 'ly',\n",
       " 'ready',\n",
       " 'looking',\n",
       " 'made',\n",
       " 'man',\n",
       " 's',\n",
       " 'song',\n",
       " 'omg',\n",
       " 'keep',\n",
       " 'yet',\n",
       " 'o',\n",
       " 'went',\n",
       " 'ever',\n",
       " 'ur',\n",
       " 'hurt',\n",
       " 'house',\n",
       " 'before',\n",
       " 'help',\n",
       " 'damn',\n",
       " 'b',\n",
       " 'thought',\n",
       " 'pretty',\n",
       " 'maybe',\n",
       " 'start',\n",
       " 'trying',\n",
       " 'sound',\n",
       " 'summer',\n",
       " 'w',\n",
       " 'd',\n",
       " 'away',\n",
       " 'old',\n",
       " 'game',\n",
       " 'finally',\n",
       " 'amazing',\n",
       " 'early',\n",
       " 'fuck',\n",
       " 'follow',\n",
       " 'r',\n",
       " 'someone',\n",
       " 'left',\n",
       " 'lost',\n",
       " 'guess',\n",
       " 'baby',\n",
       " 'rain',\n",
       " 'into',\n",
       " 'mean',\n",
       " 'big',\n",
       " 'hot',\n",
       " 'nothing',\n",
       " 't',\n",
       " 'same',\n",
       " 'missed',\n",
       " 'tell',\n",
       " 'try',\n",
       " 'other',\n",
       " 'while',\n",
       " 'bored',\n",
       " 'birthday',\n",
       " 'two',\n",
       " 'party',\n",
       " 'glad',\n",
       " 'coming',\n",
       " 'm',\n",
       " 'live',\n",
       " 'pic',\n",
       " 'weather',\n",
       " 'also',\n",
       " 'sun',\n",
       " 'mom',\n",
       " 'later',\n",
       " 'stuff',\n",
       " 'hear',\n",
       " 'play',\n",
       " 'those',\n",
       " 'th',\n",
       " 'actually',\n",
       " 'ugh',\n",
       " 'waiting',\n",
       " 'call',\n",
       " 'god',\n",
       " 'saw',\n",
       " 'world',\n",
       " 'might',\n",
       " 'exam',\n",
       " 'hard',\n",
       " 'give',\n",
       " 'car',\n",
       " 'excited',\n",
       " 'yesterday',\n",
       " 'until',\n",
       " 'said',\n",
       " 'since',\n",
       " 'thats',\n",
       " 'music',\n",
       " 'job',\n",
       " 'hi',\n",
       " 'wow',\n",
       " 'gotta',\n",
       " 'head',\n",
       " 'such',\n",
       " 'late',\n",
       " 'few',\n",
       " 'video',\n",
       " 'www',\n",
       " 'myself',\n",
       " 'around',\n",
       " 'c',\n",
       " 'monday',\n",
       " 'sunday',\n",
       " 'check',\n",
       " 'friday',\n",
       " 'many',\n",
       " 'must',\n",
       " 'cold',\n",
       " 'ndisgusting',\n",
       " 'talk',\n",
       " 'put',\n",
       " 'found',\n",
       " 'luck',\n",
       " 'boy',\n",
       " 'beautiful',\n",
       " 'follower',\n",
       " 'stop',\n",
       " 'read',\n",
       " 'making',\n",
       " 'may',\n",
       " 'end',\n",
       " 'gone',\n",
       " 'missing',\n",
       " 'their',\n",
       " 'kid',\n",
       " 'food',\n",
       " 'woke',\n",
       " 'okay',\n",
       " 'poor',\n",
       " 'least',\n",
       " 'family',\n",
       " 'leave',\n",
       " 'till',\n",
       " 'anything',\n",
       " 'tho',\n",
       " 'listening',\n",
       " 'almost',\n",
       " 'most',\n",
       " 'use',\n",
       " 'hair',\n",
       " 'funny',\n",
       " 'free',\n",
       " 'e',\n",
       " 'sweet',\n",
       " 'iphone',\n",
       " 'lunch',\n",
       " 'picture',\n",
       " 'dinner',\n",
       " 'class',\n",
       " 'month',\n",
       " 'book',\n",
       " 'eat',\n",
       " 'far',\n",
       " 'wanted',\n",
       " 'anyone',\n",
       " 'place',\n",
       " 'shit',\n",
       " 'g',\n",
       " 'cry',\n",
       " 'welcome',\n",
       " 'everything',\n",
       " 'real',\n",
       " 'forward',\n",
       " 'thinking',\n",
       " 'finished',\n",
       " 'playing',\n",
       " 'enjoy',\n",
       " 'which',\n",
       " 'believe',\n",
       " 'mine',\n",
       " 'without',\n",
       " 'win',\n",
       " 'hahaha',\n",
       " 'update',\n",
       " 'cause',\n",
       " 'idea',\n",
       " 'dad',\n",
       " 'every',\n",
       " 'stupid',\n",
       " 'buy',\n",
       " 'totally',\n",
       " 'enough',\n",
       " 'tinyurl',\n",
       " 'outside',\n",
       " 'through',\n",
       " 'ill',\n",
       " 'room',\n",
       " 'fan',\n",
       " 'these',\n",
       " 'tv',\n",
       " 'coffee',\n",
       " 'y',\n",
       " 'probably',\n",
       " 'dog',\n",
       " 'stay',\n",
       " 'saturday',\n",
       " 'xx',\n",
       " 'wrong',\n",
       " 'post',\n",
       " 'didnt',\n",
       " 'eating',\n",
       " 'once',\n",
       " 'anymore',\n",
       " 'name',\n",
       " 'minute',\n",
       " 'andgt',\n",
       " 'dream',\n",
       " 'money',\n",
       " 'busy',\n",
       " 'sooo',\n",
       " 'following',\n",
       " 'plurk',\n",
       " 'k',\n",
       " 'lovely',\n",
       " 'eye',\n",
       " 'favorite',\n",
       " 'l',\n",
       " 'headache',\n",
       " 'whole',\n",
       " 'brother',\n",
       " 'beach',\n",
       " 'run',\n",
       " 'kinda',\n",
       " 'taking',\n",
       " 'came',\n",
       " 'face',\n",
       " 'crazy',\n",
       " 'seen',\n",
       " 'super',\n",
       " 'mother',\n",
       " 'word',\n",
       " 'both',\n",
       " 'hopefully',\n",
       " 'half',\n",
       " 'hello',\n",
       " 'took',\n",
       " 'final',\n",
       " 'blog',\n",
       " 'pm',\n",
       " 'photo',\n",
       " 'hell',\n",
       " 'computer',\n",
       " 'true',\n",
       " 'reading',\n",
       " 'heart',\n",
       " 'forgot',\n",
       " 'rest',\n",
       " 'meet',\n",
       " 'goodnight',\n",
       " 'plan',\n",
       " 'rock',\n",
       " 'v',\n",
       " 'hit',\n",
       " 'leaving',\n",
       " 'able',\n",
       " 'either',\n",
       " 'sitting',\n",
       " 'else',\n",
       " 'problem',\n",
       " 'send',\n",
       " 'full',\n",
       " 'ago',\n",
       " 'used',\n",
       " 'shopping',\n",
       " 'boo',\n",
       " 'sister',\n",
       " 'part',\n",
       " 'soo',\n",
       " 'trip',\n",
       " 'seems',\n",
       " 'break',\n",
       " 'cuz',\n",
       " 'mind',\n",
       " 'change',\n",
       " 'office',\n",
       " 'alone',\n",
       " 'internet',\n",
       " 'link',\n",
       " 'tried',\n",
       " 'talking',\n",
       " 'stuck',\n",
       " 'reply',\n",
       " 'kind',\n",
       " 'raining',\n",
       " 'course',\n",
       " 'btw',\n",
       " 'remember',\n",
       " 'own',\n",
       " 'ticket',\n",
       " 'st',\n",
       " 'f',\n",
       " 'pain',\n",
       " 'heard',\n",
       " 'la',\n",
       " 'hehe',\n",
       " 'hug',\n",
       " 'facebook',\n",
       " 'care',\n",
       " 'watched',\n",
       " 'site',\n",
       " 'drink',\n",
       " 'seeing',\n",
       " 'add',\n",
       " 'dude',\n",
       " 'started',\n",
       " 'concert',\n",
       " 'cat',\n",
       " 'wake',\n",
       " 'online',\n",
       " 'wont',\n",
       " 'told',\n",
       " 'using',\n",
       " 'awake',\n",
       " 'text',\n",
       " 'fine',\n",
       " 'mileycyrus',\n",
       " 'email',\n",
       " 'quite',\n",
       " 'boring',\n",
       " 'sigh',\n",
       " 'season',\n",
       " 'open',\n",
       " 'loved',\n",
       " 'breakfast',\n",
       " 'j',\n",
       " 'study',\n",
       " 'seriously',\n",
       " 'le',\n",
       " 'sunny',\n",
       " 'person',\n",
       " 'til',\n",
       " 'hand',\n",
       " 'train',\n",
       " 'drive',\n",
       " 'pay',\n",
       " 'walk',\n",
       " 'lucky',\n",
       " 'bring',\n",
       " 'shower',\n",
       " 'bought',\n",
       " 'broke',\n",
       " 'starting',\n",
       " 'crap',\n",
       " 'hungry',\n",
       " 'june',\n",
       " 'anyway',\n",
       " 'turn',\n",
       " 'as',\n",
       " 'lady',\n",
       " 'called',\n",
       " 'asleep',\n",
       " 'bye',\n",
       " 'min',\n",
       " 'test',\n",
       " 'heading',\n",
       " 'red',\n",
       " 'h',\n",
       " 'afternon',\n",
       " 'xd',\n",
       " 'sleeping',\n",
       " 'move',\n",
       " 'instead',\n",
       " 'homdisgusting',\n",
       " 'album',\n",
       " 'youtube',\n",
       " 'second',\n",
       " 'jealous',\n",
       " 'meeting',\n",
       " 'yea',\n",
       " 'ice',\n",
       " 'story',\n",
       " 'reason',\n",
       " 'sore',\n",
       " 'page',\n",
       " 'wonderful',\n",
       " 'mr',\n",
       " 'city',\n",
       " 'enjoying',\n",
       " 'mad',\n",
       " 'finish',\n",
       " 'high',\n",
       " 'set',\n",
       " 'running',\n",
       " 'holiday',\n",
       " 'bout',\n",
       " 're',\n",
       " 'together',\n",
       " 'cut',\n",
       " 'fm',\n",
       " 'hoping',\n",
       " 'tommcfly',\n",
       " 'definitely',\n",
       " 'top',\n",
       " 'soooo',\n",
       " 'laptop',\n",
       " 'message',\n",
       " 'dead',\n",
       " 'star',\n",
       " 'ask',\n",
       " 'co',\n",
       " 'write',\n",
       " 'store',\n",
       " 'sometimes',\n",
       " 'moment',\n",
       " 'evening',\n",
       " 'town',\n",
       " 'couple',\n",
       " 'nap',\n",
       " 'short',\n",
       " 'church',\n",
       " 'congrats',\n",
       " 'award',\n",
       " 'tour',\n",
       " 'died',\n",
       " 'dear',\n",
       " 'side',\n",
       " 'fail',\n",
       " 'tea',\n",
       " 'happened',\n",
       " 'water',\n",
       " 'foot',\n",
       " 'ipod',\n",
       " 'won',\n",
       " 'fall',\n",
       " 'smile',\n",
       " 'point',\n",
       " 'line',\n",
       " 'close',\n",
       " 'worry',\n",
       " 'lil',\n",
       " 'loving',\n",
       " 'hr',\n",
       " 'ive',\n",
       " 'uk',\n",
       " 'visit',\n",
       " 'gym',\n",
       " 'goin',\n",
       " 'weird',\n",
       " 'mood',\n",
       " 'ride',\n",
       " 'listen',\n",
       " 'perfect',\n",
       " 'date',\n",
       " 'ppl',\n",
       " 'ddlovato',\n",
       " 'happen',\n",
       " 'dance',\n",
       " 'studying',\n",
       " 'cream',\n",
       " 'list',\n",
       " 'hang',\n",
       " 'mum',\n",
       " 'chocolate',\n",
       " 'math',\n",
       " 'blip',\n",
       " 'fb',\n",
       " 'interesting',\n",
       " 'catch',\n",
       " 'account',\n",
       " 'clean',\n",
       " 'agree',\n",
       " 'band',\n",
       " 'english',\n",
       " 'seem',\n",
       " 'da',\n",
       " 'episode',\n",
       " 'writing',\n",
       " 'wonder',\n",
       " 'ate',\n",
       " 'throat',\n",
       " 'air',\n",
       " 'myspace',\n",
       " 'saying',\n",
       " 'broken',\n",
       " 'london',\n",
       " 'flight',\n",
       " 'pool',\n",
       " 'vote',\n",
       " 'worst',\n",
       " 'wedding',\n",
       " 'xxx',\n",
       " 'wishing',\n",
       " 'black',\n",
       " 'unfortunately',\n",
       " 'supposed',\n",
       " 'knew',\n",
       " 'worth',\n",
       " 'chance',\n",
       " 'window',\n",
       " 'fast',\n",
       " 'team',\n",
       " 'three',\n",
       " 'sunshine',\n",
       " 'park',\n",
       " 'hmm',\n",
       " 'via',\n",
       " 'green',\n",
       " 'sat',\n",
       " 'jonas',\n",
       " 'driving',\n",
       " 'moving',\n",
       " 'cleaning',\n",
       " 'em',\n",
       " 'card',\n",
       " 'sleepy',\n",
       " 'question',\n",
       " 'pick',\n",
       " 'sent',\n",
       " 'past',\n",
       " 'blue',\n",
       " 'comment',\n",
       " 'shirt',\n",
       " 'tweeting',\n",
       " 'horrible',\n",
       " 'mac',\n",
       " 'followfriday',\n",
       " 'parent',\n",
       " 'website',\n",
       " 'beer',\n",
       " 'forget',\n",
       " 'number',\n",
       " 'college',\n",
       " 'understand',\n",
       " 'worse',\n",
       " 'drinking',\n",
       " 'david',\n",
       " 'gave',\n",
       " 'bus',\n",
       " 'paper',\n",
       " 'upset',\n",
       " 'bet',\n",
       " 'answer',\n",
       " 'slow',\n",
       " 'cake',\n",
       " 'leg',\n",
       " 'under',\n",
       " 'finger',\n",
       " 'easy',\n",
       " 'whats',\n",
       " 'bday',\n",
       " 'special',\n",
       " 'apparently',\n",
       " 'tuesday',\n",
       " 'doesnt',\n",
       " 'moon',\n",
       " 'scared',\n",
       " 'canot',\n",
       " 'mtv',\n",
       " 'project',\n",
       " 'shoe',\n",
       " 'rather',\n",
       " 'wtf',\n",
       " 'due',\n",
       " 'body',\n",
       " 'vacation',\n",
       " 'dress',\n",
       " 'huge',\n",
       " 'miley',\n",
       " 'tom',\n",
       " 'white',\n",
       " 'beat',\n",
       " 'longer',\n",
       " 'shop',\n",
       " 'flu',\n",
       " 'plus',\n",
       " 'fell',\n",
       " 'nope',\n",
       " 'hanging',\n",
       " 'spend',\n",
       " 'thursday',\n",
       " 'wear',\n",
       " 'spent',\n",
       " 'shame',\n",
       " 'yr',\n",
       " 'wondering',\n",
       " 'thx',\n",
       " 'earlier',\n",
       " 'kill',\n",
       " 'load',\n",
       " 'nd',\n",
       " 'support',\n",
       " 'join',\n",
       " 'voice',\n",
       " 'babe',\n",
       " 'woman',\n",
       " 'cheer',\n",
       " 'sims',\n",
       " 'aww',\n",
       " 'age',\n",
       " 'si',\n",
       " 'laugh',\n",
       " 'lazy',\n",
       " 'light',\n",
       " 'shot',\n",
       " 'chat',\n",
       " 'figure',\n",
       " 'dm',\n",
       " 'boyfriend',\n",
       " 'stomach',\n",
       " 'power',\n",
       " 'forever',\n",
       " 'learn',\n",
       " 'camera',\n",
       " 'apple',\n",
       " 'july',\n",
       " 'especially',\n",
       " 'sadly',\n",
       " 'garden',\n",
       " 'cd',\n",
       " 'during',\n",
       " 'warm',\n",
       " 'fair',\n",
       " 'radio',\n",
       " 'ff',\n",
       " 'pizza',\n",
       " 'slept',\n",
       " 'box',\n",
       " 'small',\n",
       " 'airport',\n",
       " 'bike',\n",
       " 'sign',\n",
       " 'father',\n",
       " 'rainy',\n",
       " 'luv',\n",
       " 'safe',\n",
       " 'google',\n",
       " 'jonasbrothers',\n",
       " 'yummy',\n",
       " 'meant',\n",
       " 'met',\n",
       " 'bitch',\n",
       " 'different',\n",
       " 'case',\n",
       " 'bbq',\n",
       " 'service',\n",
       " 'except',\n",
       " 'save',\n",
       " 'looked',\n",
       " 'inside',\n",
       " 'liked',\n",
       " 'graduation',\n",
       " 'cousin',\n",
       " 'chicken',\n",
       " 'cup',\n",
       " 'tummy',\n",
       " 'note',\n",
       " 'club',\n",
       " 'havent',\n",
       " 'die',\n",
       " 'road',\n",
       " 'fix',\n",
       " 'officially',\n",
       " 'shall',\n",
       " 'worked',\n",
       " 'son',\n",
       " 'fly',\n",
       " 'felt',\n",
       " 'film',\n",
       " 'doctor',\n",
       " 'z',\n",
       " 'absolutely',\n",
       " 'hospital',\n",
       " 'order',\n",
       " 'share',\n",
       " 'twilight',\n",
       " 'kiss',\n",
       " 'yo',\n",
       " 'mail',\n",
       " 'ma',\n",
       " 'played',\n",
       " 'each',\n",
       " 'hill',\n",
       " 'yourself',\n",
       " 'decided',\n",
       " 'wine',\n",
       " 'french',\n",
       " 'needed',\n",
       " 'goodbye',\n",
       " 'xoxo',\n",
       " 'exciting',\n",
       " 'gorgeous',\n",
       " 'alright',\n",
       " 'issue',\n",
       " 'lonely',\n",
       " 'proud',\n",
       " 'anoying',\n",
       " 'hubby',\n",
       " 'fact',\n",
       " 'pink',\n",
       " 'wednesday',\n",
       " 'bro',\n",
       " 'mommy',\n",
       " 'storm',\n",
       " 'living',\n",
       " 'bar',\n",
       " 'hmmm',\n",
       " 'exactly',\n",
       " 'taken',\n",
       " 'touch',\n",
       " 'ouch',\n",
       " 'nick',\n",
       " 'freaking',\n",
       " 'glass',\n",
       " 'blah',\n",
       " 'ball',\n",
       " 'wit',\n",
       " 'wife',\n",
       " 'bag',\n",
       " 'smell',\n",
       " 'de',\n",
       " 'whatever',\n",
       " 'front',\n",
       " 'turned',\n",
       " 'dvd',\n",
       " 'argh',\n",
       " 'lame',\n",
       " 'download',\n",
       " 'happens',\n",
       " 'company',\n",
       " 'near',\n",
       " 'behind',\n",
       " 'clothes',\n",
       " 'daughter',\n",
       " 'somdisgusting',\n",
       " 'scary',\n",
       " 'sold',\n",
       " 'profile',\n",
       " 'mate',\n",
       " 'jus',\n",
       " 'gd',\n",
       " 'business',\n",
       " 'packing',\n",
       " 'version',\n",
       " 'mile',\n",
       " 'ear',\n",
       " 'drunk',\n",
       " 'bb',\n",
       " 'none',\n",
       " 'q',\n",
       " 'pas',\n",
       " 'everybody',\n",
       " 'killing',\n",
       " 'state',\n",
       " 'deal',\n",
       " 'door',\n",
       " 'single',\n",
       " 'although',\n",
       " 'realized',\n",
       " 'matter',\n",
       " 'giving',\n",
       " 'round',\n",
       " 'waking',\n",
       " 'country',\n",
       " 'alot',\n",
       " 'yours',\n",
       " 'hahah',\n",
       " 'vega',\n",
       " 'awww',\n",
       " 'lose',\n",
       " 'web',\n",
       " 'guitar',\n",
       " 'isnt',\n",
       " 'walking',\n",
       " 'app',\n",
       " 'puppy',\n",
       " 'relaxing',\n",
       " 'self',\n",
       " 'hold',\n",
       " 'sale',\n",
       " 'taste',\n",
       " 'art',\n",
       " 'along',\n",
       " 'event',\n",
       " 'group',\n",
       " 'plane',\n",
       " 'death',\n",
       " 'interview',\n",
       " 'child',\n",
       " 'staying',\n",
       " 'eh',\n",
       " 'fit',\n",
       " 'drop',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.wv.index_to_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88326489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vector_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c1660d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0617004 ,  0.15368834, -0.30466482, -0.09091772, -0.29671198,\n",
       "       -0.32473722, -0.12110214,  0.5677969 , -0.06950313, -0.04396479,\n",
       "       -0.30846474, -0.19760323, -0.00111749,  0.18611506,  0.08066562,\n",
       "       -0.5225079 , -0.42478046, -0.5013711 , -0.0543109 , -0.25763696,\n",
       "        0.2583714 ,  0.15878998,  0.43007207,  0.25645196,  0.23807997,\n",
       "        0.24688737, -0.6870859 ,  0.14629991, -0.23371446,  0.07841577,\n",
       "        0.1480483 , -0.02460455,  0.47732973, -0.37960696, -0.02724978,\n",
       "        0.41526946, -0.02428965, -0.06622016, -0.21353182, -0.9388725 ,\n",
       "       -0.01753375, -0.07331342, -0.10368375,  0.6707508 ,  0.02422952,\n",
       "       -0.6419385 ,  0.31703216,  0.02765011,  0.66191643, -0.08976927,\n",
       "       -0.03277675, -0.0918693 ,  0.5167764 , -0.12165214,  0.38509688,\n",
       "        0.34030005, -0.08161294, -0.42406213, -0.17276758,  0.22395438,\n",
       "       -0.5836187 ,  0.18239118, -0.26240957,  0.3153187 ,  0.1487198 ,\n",
       "        0.14722373,  0.1810682 ,  0.3247819 ,  0.09473322,  0.04722451,\n",
       "        0.46975905,  0.08181176,  0.13769555,  0.24801625, -0.11850017,\n",
       "        0.40149423,  0.07308754, -0.35274932, -0.5381021 , -0.4709417 ,\n",
       "       -0.32851088, -0.22340432, -0.33601567,  1.1010852 ,  0.29393718,\n",
       "        0.22644486, -0.06131136, -0.01824161,  0.0191519 ,  0.12161774,\n",
       "        0.23119062,  0.12677412, -0.07995478,  0.03065276,  0.7516613 ,\n",
       "        0.09254586, -0.07924353,  0.08777277, -0.31605995, -0.32575414],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.get_vector('Phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebf5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3db2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee0717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071402d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342bbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0545c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b523546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'] = X_train['text'].apply(advanced_data_cleaning)\n",
    "X_val['text'] = X_val['text'].apply(advanced_data_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7387cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train['target'] = le.fit_transform(y_train['target'])\n",
    "y_val['target'] = le.transform(y_val['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34547caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenized = [[w for w in sentence.split(\" \") if w != \"\"] for sentence in X_train['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c9376f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(x_tokenized,\n",
    "                 vector_size=100\n",
    "                 # Size is the length of our vector.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "685e07bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x290ff49d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6122883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequencer():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 all_words,\n",
    "                 max_words,\n",
    "                 seq_len,\n",
    "                 embedding_matrix\n",
    "                ):\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.embed_matrix = embedding_matrix\n",
    "        \"\"\"\n",
    "        temp_vocab = Vocab which has all the unique words\n",
    "        self.vocab = Our last vocab which has only most used N words.\n",
    "    \n",
    "        \"\"\"\n",
    "        temp_vocab = list(set(all_words))\n",
    "        self.vocab = []\n",
    "        self.word_cnts = {}\n",
    "        \"\"\"\n",
    "        Now we'll create a hash map (dict) which includes words and their occurencies\n",
    "        \"\"\"\n",
    "        for word in temp_vocab:\n",
    "            # 0 does not have a meaning, you can add the word to the list\n",
    "            # or something different.\n",
    "            count = len([0 for w in all_words if w == word])\n",
    "            self.word_cnts[word] = count\n",
    "            counts = list(self.word_cnts.values())\n",
    "            indexes = list(range(len(counts)))\n",
    "        \n",
    "        # Now we'll sort counts and while sorting them also will sort indexes.\n",
    "        # We'll use those indexes to find most used N word.\n",
    "        cnt = 0\n",
    "        while cnt + 1 != len(counts):\n",
    "            cnt = 0\n",
    "            for i in range(len(counts)-1):\n",
    "                if counts[i] < counts[i+1]:\n",
    "                    counts[i+1],counts[i] = counts[i],counts[i+1]\n",
    "                    indexes[i],indexes[i+1] = indexes[i+1],indexes[i]\n",
    "                else:\n",
    "                    cnt += 1\n",
    "        \n",
    "        for ind in indexes[:max_words]:\n",
    "            self.vocab.append(temp_vocab[ind])\n",
    "                    \n",
    "    def textToVector(self,text):\n",
    "        # First we need to split the text into its tokens and learn the length\n",
    "        # If length is shorter than the max len we'll add some spaces (100D vectors which has only zero values)\n",
    "        # If it's longer than the max len we'll trim from the end.\n",
    "        tokens = text.split()\n",
    "        len_v = len(tokens)-1 if len(tokens) < self.seq_len else self.seq_len-1\n",
    "        vec = []\n",
    "        for tok in tokens[:len_v]:\n",
    "            try:\n",
    "                vec.append(self.embed_matrix[tok])\n",
    "            except Exception as E:\n",
    "                pass\n",
    "        \n",
    "        last_pieces = self.seq_len - len(vec)\n",
    "        for i in range(last_pieces):\n",
    "            vec.append(np.zeros(100,))\n",
    "        \n",
    "        return np.asarray(vec).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67cc5f12",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sequencer \u001b[38;5;241m=\u001b[39m \u001b[43mSequencer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_words\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_tokenized\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmax_words\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m              \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [12], line 26\u001b[0m, in \u001b[0;36mSequencer.__init__\u001b[0;34m(self, all_words, max_words, seq_len, embedding_matrix)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mNow we'll create a hash map (dict) which includes words and their occurencies\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m temp_vocab:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# 0 does not have a meaning, you can add the word to the list\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# or something different.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m all_words \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;241m==\u001b[39m word])\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_cnts[word] \u001b[38;5;241m=\u001b[39m count\n\u001b[1;32m     28\u001b[0m     counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_cnts\u001b[38;5;241m.\u001b[39mvalues())\n",
      "Cell \u001b[0;32mIn [12], line 26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mNow we'll create a hash map (dict) which includes words and their occurencies\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m temp_vocab:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# 0 does not have a meaning, you can add the word to the list\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# or something different.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m all_words \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;241m==\u001b[39m word])\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_cnts[word] \u001b[38;5;241m=\u001b[39m count\n\u001b[1;32m     28\u001b[0m     counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_cnts\u001b[38;5;241m.\u001b[39mvalues())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sequencer = Sequencer(all_words = [token for seq in x_tokenized for token in seq],\n",
    "              max_words = 1200,\n",
    "              seq_len = 15,\n",
    "              embedding_matrix = model.wv\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d7d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = sequencer.textToVector(\"i am in love with you\")\n",
    "test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc1322",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579a64d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
